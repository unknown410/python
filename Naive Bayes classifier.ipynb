{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task.In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "For example,a dress may be considered to be a shirt if it is red, printed, and has a sleeve. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this cloth is a shirt and that is why it is known as ‘Naive’.\n",
    "\n",
    "Given a Hypothesis H and evidence E, Bayes’ Theorem states that the relationship between the probability of Hypothesis before getting the evidence P(H) and the probability of the hypothesis after getting the evidence P(H|E) is :\n",
    "\n",
    "P(H|E) = P(E|H).P(H)/P(E)\n",
    "\n",
    "This relates the probability of the hypothesis before getting the evidence P(H), to the probability of the hypothesis after getting the evidence,\n",
    "\n",
    "P(H|E). For this reason,  is called the prior probability, while P(H|E) is called the posterior probability. The factor that relates the two, P(H|E) / P(E), is called the likelihood ratio. Using these terms, Bayes’ theorem can be rephrased as:\n",
    "\n",
    " \n",
    "“The posterior probability equals the prior probability times the likelihood ratio.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the case of using NB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are working on a classification problem and you have generated your set of hypothesis, created features and discussed the importance of variables. Within an hour, stakeholders want to see the first cut of the model.\n",
    "\n",
    "What will you do? You have hunderds of thousands of data points and quite a few variables in your training data set. In such situation, if I were at your place, I would have used ‘Naive Bayes‘, which can be extremely fast relative to other classification algorithms. It works on Bayes theorem of probability to predict the class of unknown data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let’s see where is it used in the Industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News Categorization, Weather Prediction,Medical Diagnosis, Spam Filtering, Face recognition, Digit recognition...etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE:\n",
    "Let's set out on a journey by train to create our first very simple Naive Bayes Classifier. Let us assume we are in the city of Hamburg and we want to travel to Munich. We will have to change trains in Frankfurt am Main. We know from previous train journeys that our train from Hamburg might be delayed and the we will not catch our connecting train in Frankfurt. The probability that we will not be in time for our connecting train depends on how high our possible delay will be. The connecting train will not wait for more than five minutes. Sometimes the other train is delayed as well.\n",
    "\n",
    "The following lists 'in_time' (the train from Hamburg arrived in time to catch the connecting train to Munich) and 'too_late' (connecting train is missed) are data showing the situation over some weeks. The first component of each tuple shows the minutes the train was late and the second component shows the number of time this occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 22),\n",
       " (1, 19),\n",
       " (2, 17),\n",
       " (3, 18),\n",
       " (4, 16),\n",
       " (5, 15),\n",
       " (6, 9),\n",
       " (7, 7),\n",
       " (8, 4),\n",
       " (9, 3),\n",
       " (10, 3),\n",
       " (11, 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tuples consist of (delay time of train1, number of times)\n",
    "# tuples are (minutes, number of times)\n",
    "in_time = [(0, 22), (1, 19), (2, 17), (3, 18),\n",
    "           (4, 16), (5, 15), (6, 9), (7, 7),\n",
    "           (8, 4), (9, 3), (10, 3), (11, 2)]\n",
    "in_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 6),\n",
       " (7, 9),\n",
       " (8, 12),\n",
       " (9, 17),\n",
       " (10, 18),\n",
       " (11, 15),\n",
       " (12, 16),\n",
       " (13, 7),\n",
       " (14, 8),\n",
       " (15, 5)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "too_late = [(6, 6), (7, 9), (8, 12), (9, 17), \n",
    "            (10, 18), (11, 15), (12,16), (13, 7),\n",
    "            (14, 8), (15, 5)]\n",
    "too_late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "X, Y = zip(*in_time)\n",
    "X2, Y2 = zip(*too_late)\n",
    "bar_width = 0.9\n",
    "plt.bar(X, Y, bar_width,  color=\"blue\", alpha=0.75, label=\"in time\")\n",
    "bar_width = 0.8\n",
    "plt.bar(X2, Y2, bar_width,  color=\"red\", alpha=0.75, label=\"too late\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data we can deduce that the probability of catching the connecting train if we are one minute late is 1, because we had 19 successful cases experienced and no misses, i.e. there is no tuple with 1 as the first component in 'too_late'.\n",
    "\n",
    "We will denote the event \"train arrived in time to catch the connecting train\" with S (success) and the 'unlucky' event \"train arrived too late to catch the connecting train\" with M (miss)\n",
    "\n",
    "We can now define the probability \"catching the train given that we are 1 minute late\" formally:\n",
    "\n",
    "P(S|1)=19/19=1\n",
    "We used the fact that the tuple (1,19) is in 'in_time' and there is no tuple with the first component 1 in 'too_late'\n",
    "\n",
    "It's getting critical for catching the connecting train to Munich, if we are 6 minutes late. Yet, the chances are still 60 %:\n",
    "\n",
    "P(S|6)=9/9+6=0.6\n",
    "Accordingly, the probability for missing the train knowing that we are 6 minutes late is:\n",
    "\n",
    "P(M|6)=6/9+6=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 0\n",
      "0 1.0\n",
      "1 1.0\n",
      "2 1.0\n",
      "3 1.0\n",
      "4 1.0\n",
      "5 1.0\n",
      "6 0.6\n",
      "7 0.4375\n",
      "8 0.25\n",
      "9 0.15\n",
      "10 0.14285714285714285\n",
      "11 0.11764705882352941\n",
      "12 0\n"
     ]
    }
   ],
   "source": [
    "#We can write a 'classifier' function, which will give the probability for catching the connecting train:\n",
    "\n",
    "in_time_dict = dict(in_time)\n",
    "too_late_dict = dict(too_late)\n",
    "def catch_the_train(min):\n",
    "    s = in_time_dict.get(min, 0)\n",
    "    if s == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        m = too_late_dict.get(min, 0)\n",
    "        return s / (s + m)\n",
    "for minutes in range(-1, 13):\n",
    "    print(minutes, catch_the_train(minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOTHER EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning features and label variables\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n",
    "\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "weather_encoded=le.fit_transform(weather)\n",
    "print(wheather_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp: [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "Play: [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#similarly encode temp and play columns\n",
    "# Converting string labels into numbers\n",
    "temp_encoded=le.fit_transform(temp)\n",
    "label=le.fit_transform(play)\n",
    "print(\"Temp:\",temp_encoded)\n",
    "print(\"Play:\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (2, 1), (0, 1), (1, 2), (1, 0), (1, 0), (0, 0), (2, 2), (2, 0), (1, 2), (2, 2), (0, 2), (0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "#Combinig weather and temp into single listof tuples\n",
    "features=list(zip(weather_encoded,temp_encoded))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: [1]\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features,label)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\n",
    "print(\"Predicted Value:\", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 1 indicates that players can 'play'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES WITH MULTIPLE LABELS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we will be using wine dataset which is a very famous multi-class classification problem. \"This dataset is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\"\n",
    "\n",
    "Dataset comprises of 13 features (alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline) and type of wine cultivar.\n",
    "\n",
    "This data has three type of wine Class_0, Class_1, and Class_3. Here you can build a model to classify the type of wine.The dataset is available in the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels:  ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# print the names of the 13 features\n",
    "print(\"Features: \", wine.feature_names)\n",
    "\n",
    "# print the label type of wine(class_0, class_1, class_2)\n",
    "print(\"Labels: \", wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the wine labels (0:Class_0, 1:class_2, 2:class_2)\n",
    "print(wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ITSpark\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9074074074074074\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantages of NB:\n",
    "It is not only a simple approach but also a fast and accurate method for prediction.\n",
    "Naive Bayes has very low computation cost.\n",
    "It can efficiently work on a large dataset.\n",
    "It performs well in case of discrete response variable compared to the continuous variable.\n",
    "It can be used with multiple class prediction problems.\n",
    "It also performs well in the case of text analytics problems.\n",
    "When the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Disadvantages of NB:\n",
    "The assumption of independent features. In practice, it is almost impossible that model will get a set of predictors which are entirely independent.\n",
    "If there is no training tuple of a particular class, this causes zero posterior probability. In this case, the model is unable to make predictions. This problem is known as Zero Probability/Frequency Problem."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CONCLUSION:\n",
    "Naive Bayes is the most straightforward and most potent algorithm. In spite of the significant advances of Machine Learning in the last couple of years, it has proved its worth. It has been successfully deployed in many applications from text analytics to recommendation engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
